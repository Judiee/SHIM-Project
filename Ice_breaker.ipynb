{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Ice Breaker\n",
    "\n",
    "So this is just the process i used to disintegrate the data that was given to what we want to work with. This isn't scalable because it's quite tacky and i believe i have done some unpythonic things inbetween but we know that the data given isn't even structured. So for now, this is what i am working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, glob\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(\"*.xlsx\"))    \n",
    "all_df = {}\n",
    "for f in all_files:\n",
    "    df = pd.read_excel(f,sheet_name=None) # set sheet_name to \"None\" to access every sheet in that workbook as a dictionary \n",
    "    all_df[f] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glob function return a list of paths matching a pathname pattern.\n",
    "Then using a for loop, read the files and store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A dictionary for storing the comments for each location\n",
    "# A list for storing each comment from each iteration\n",
    "comments = []\n",
    "comment_list = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inpatient Survey Analysis 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the dictionary we saved above we want to read the first values which is for the IPD 2019. could have figured out how to iterate through the dictionary but didn't wanna waste time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Feb 2019', 'March 2019', 'April 2019', 'May 2019', 'June 2019', 'July 2019', 'August 2019', 'September 2019', 'October 2019', 'November 2019', 'December 2019'])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Workbook in the dictionary\n",
    "work1 = all_df.get('Inpatient Survey Analysis 2019.xlsx')\n",
    "work1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Remember the sheet_name we set to none in the begining, the keys printed out are the sheets in the workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So after studying the workbook, i discovered that there are two tables in one and anything that applied to one applies to the other across all sheets and workbook because they store same length and definition of informations. \n",
    "\n",
    "So my first approach is to divide each sheet into two and iterate through each half of everysheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan 2020 Usefulness of leaflets provided\n",
      "March 2020 Usefulness of leaflets provided\n"
     ]
    }
   ],
   "source": [
    "# 1st half of the work book for each sheet\n",
    "path = \"data/\"\n",
    "ext = \".xlsx\"\n",
    "survey_dict = {}\n",
    "resp1 = 0\n",
    "dis1 = 0\n",
    "for key in work1.keys():\n",
    "    loc_1 = work1[key].loc[:,\"INPATIENT SATISFACTION SURVEY\":\"Unnamed: 9\"]\n",
    "    df_list = loc_1.transpose().values.tolist()\n",
    "    loc = df_list[0][0]\n",
    "    location = loc.title().split()[1]\n",
    "    discharges = df_list[4][1]\n",
    "    respondents = df_list[4][2]\n",
    "    survey_stats = df_list[2][5:29]\n",
    "    excel_5 = df_list[3][5:29]\n",
    "    good_4 = df_list[4][5:29]\n",
    "    avg_3 = df_list[5][5:29]\n",
    "    poor_2 = df_list[6][5:29]\n",
    "    no_resp = df_list[7][5:29]\n",
    "    comment = df_list[3][31:]\n",
    "    comments.append(comment)\n",
    "    for a in range(len(survey_stats)):\n",
    "        one = ([5]*excel_5[a])+([4]*good_4[a])+([3]*avg_3[a])+([2]*poor_2[a])+([np.nan]*no_resp[a])\n",
    "        if len(one) == 0:\n",
    "            print(key,survey_stats[a])\n",
    "            one = [np.nan]*respondents\n",
    "            location = \"Bourdillion\"\n",
    "            #one.append(one_)\n",
    "        if len(one) == respondents:\n",
    "            reshuffle_one = random.sample(one, len(one))\n",
    "        elif len(one) > respondents:\n",
    "            diff = len(one) - respondents \n",
    "            reshuffle_one = random.sample(one, len(one)-diff)\n",
    "        locate = len(one)* [location]\n",
    "        date = len(one)* [key]\n",
    "        survey_dict[\"Date\"] = date\n",
    "        survey_dict[\"Location\"] = locate\n",
    "        survey_dict[survey_stats[a]] = reshuffle_one\n",
    "    df = pd.DataFrame(survey_dict)\n",
    "    df.to_excel(path+key+\"_1\"+ext,index=False)\n",
    "    comment_list.update({location : comments})\n",
    "    resp1 += respondents\n",
    "    dis1 += discharges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The print out above is just for me to check that my adjustments for anormaly works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 2019 Usefulness of leaflets provided\n",
      "April 2019 Usefulness of leaflets provided\n",
      "June 2019 Usefulness of leaflets provided\n",
      "July 2019 Usefulness of leaflets provided\n",
      "August 2019 Usefulness of leaflets provided\n",
      "November 2019 Usefulness of leaflets provided\n",
      "December 2019 Efficiency of billings process\n"
     ]
    }
   ],
   "source": [
    "# 2nd half of the work book for each sheet\n",
    "path = \"data/\"\n",
    "ext = \".xlsx\"\n",
    "survey_dict = {}\n",
    "resp2 = 0\n",
    "dis2 = 0\n",
    "for key in work1.keys():\n",
    "    loc_2 = work1[key].loc[:,\"INPATIENT SATISFACTION SURVEY.1\":]\n",
    "    df_list = loc_2.transpose().values.tolist()\n",
    "    loc = df_list[0][0]\n",
    "    location = loc.title().split()[1]\n",
    "    discharges = df_list[4][1]\n",
    "    respondents = df_list[4][2]\n",
    "    survey_stats = df_list[2][5:29]\n",
    "    excel_5 = df_list[3][5:29]\n",
    "    good_4 = df_list[4][5:29]\n",
    "    avg_3 = df_list[5][5:29]\n",
    "    poor_2 = df_list[6][5:29]\n",
    "    no_resp = df_list[7][5:29]\n",
    "    comment = df_list[3][31:]\n",
    "    comments.append(comment)\n",
    "    survey_dict = {}\n",
    "    for a in range(len(survey_stats)):\n",
    "        one = ([5]*excel_5[a])+([4]*good_4[a])+([3]*avg_3[a])+([2]*poor_2[a])+([np.nan]*no_resp[a])\n",
    "        if len(one) < respondents:\n",
    "            print(key,survey_stats[a])\n",
    "            one = [np.nan]*respondents\n",
    "            location = \"Bourdillion\"\n",
    "        if len(one) == respondents:\n",
    "            reshuffle_one = random.sample(one, len(one))\n",
    "        elif len(one) > respondents:\n",
    "            diff = len(one) - respondents \n",
    "            reshuffle_one = random.sample(one, len(one)-diff)\n",
    "        locate = len(one)* [location]\n",
    "        date = len(one)* [key]\n",
    "        survey_dict[\"Date\"] = date\n",
    "        survey_dict[\"Location\"] = locate\n",
    "        survey_dict[survey_stats[a]] = reshuffle_one\n",
    "    df1 = pd.DataFrame(survey_dict)\n",
    "    df1.to_excel(path+key+\"_2\"+ext,index=False)\n",
    "    comment_list.update({location : comments})\n",
    "    resp2 += respondents\n",
    "    dis2 += discharges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inpatient Survey Analysis 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this point i am done for the first workbook, so i just apply same code to the other workbook by changing the line below to the next file in the dictionary which is 'Inpatient Survey Analysis 2020.xlsx' and run the process again.\n",
    "\n",
    "    work1 = all_df.get('Inpatient Survey Analysis 2019.xlsx')\n",
    "    work1.keys()\n",
    "   \n",
    "**NB:** Do not rerun the cell that initializes the comment list and comment dictionary during this second process so you don't overwrite the content in them. Only is you are starting the whole process afresh can you rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sum it all Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the comment dictionary to a txt file. \n",
    "#json.dump(comment_list, open(\"data/IPD_Survey_Comments.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this point i have done the process for the two workbooks and i have the files written to my folder on my local directory. Because they are all one information, i want to couple them all up so  i employ my glob function and with a for loop i read the files then concatenate them into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2648, 26)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_files = glob.glob(os.path.join(\"data/\", \"*.xlsx\"))\n",
    "new_df = []\n",
    "for f in add_files:\n",
    "    df = pd.read_excel(f)\n",
    "    new_df.append(df)\n",
    "    \n",
    "merged_df = pd.concat(new_df,axis=0)\n",
    "#merged_df.to_csv(\"data/IPD_Survey_Response.csv\")\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
